# Chapter 1

## Keywords:
**LLMs**, **transformers**, **fine-tune**, **model-parameters**, **parameter weights**,



LLMs are like your swiss army knief. Another analogy would be to, LLMs are like your intern who read the entire docs- some one who has seen entire data set, reasonably well at answering about everything, but not an expert in anything unless properly trained/ finetuned etc. Therefore, finetuning/ training still trickles down to human. Like your intern is ready to work on anything, LLMs can also be used to any specific task. This is unlike to previous situations where we had to write *custom NLP programs* for each type of task (e.g. text categorization, language translation etc.)



## What's an LLM

LLMs hare human *like* capability to understand, generate, and interpret text. This is because they are *trained* on vast amount of data-typically the entire internet. As a result, LLMs have seen more- or in otherwords, LLMs have deeper/ rich contextual information (e.g. bank: money vs river). Hence, LLMs typically outperform a wide array of NLP tasks such as text translationn, sentiment analysis, QnA etc.  

LLMs have billions and trillions of parameters. These parameters have **weights**, which are adjustable. 

LLMs leverage special kind of deep neural network architecture called **transformers**. Transformers has enabled textual data training by allowing the models to *capture* wide array of important elements in text processing/ NLP. These include, linguist naunces, contexts, patterns...etc. The most special thing about **transformers** is that, it can *pay attention to the selected parts in the text (hence called selective attention)* when making *predictions*. 

#### Tiny Language Models (TLM)

