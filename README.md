# AI Diary
Creating this repo to keep track of my AI journey

# Table of Contents 




# Resources/ Tutorials 
1. [fast.ai](https://course.fast.ai/) - to learn Machine Learning, Deep Learning through hands on work
2. Stanford’s cs231n Computer Vision lecture series by Andrej Karparthy
3. [Eureka Labs](https://eurekalabs.ai/) by [Andrej Karparthy](https://karpathy.ai/)
4. Sebastian Rachka’s resources
    - Book: [Machine Learning with PyTorch and Scikit-Learn](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)
    - Book: [Build a Large Language model (from Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
5. [Hamel Husain](https://hamel.dev/blog/posts/course/) - An Open Course on LLMs, Led by Practitioners
6. [EleutherAI](https://www.eleuther.ai/) - 
7. Creative tools: https://pharmapsychotic.com/tools.html 
8. Jhono https://www.youtube.com/@datasciencecastnet
9. [StaStanford CS229 I Machine Learning I Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts)
    - A Highly rated video
10. [CIS 7000](https://llm-class.github.io/) 
    - Free course from PhDs and industry leaders
11. [Stanford CS224N NLP with Deep Learning | 2023 | Hugging Face Tutorial, Eric Frankel](https://www.youtube.com/watch?v=b80by3Xk_A8)
    - NLP crashcourse with transformers

# Projects
## 1. F1 Car Classifier 
## 2. Fine tune Flux.1-dev Stable Diffusion
## 3. Multi-Agentic application with CrewAI  

# To learn/ Interests (List of Todos)
1. [Google AI Studio](https://aistudio.google.com/)
2. NER for Tech KeyNotes. 
3. Zuckerberg case-study with NLP.
4. ~Analyze *#chip* race~. (found that this has already been done)
5. Data Center and Energy. 
6. Sentiment Analysis project for Williams. 
7. Super car comparison (???)
8. Compare and contrast Free versions of LLMs on a *useful* task. 
9. Master following tools **Amazon SageMaker**, **Google Vertex AI**, **CursorAI**, **Windsurf**. 

# Diary

## 2025

### January

**7th January 2024**

1. [StaStanford CS229 I Machine Learning I Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts)
    - Overfitting is really good when you build your model on high quality data (eg. Wikipedia)

**5th January 2024**
1. Started reading  [Machine Learning with PyTorch and Scikit-Learn](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312) by Sebastian Rachka. 

**2nd January 2024**

1. Started using [DeepSeek](https://www.deepseek.com/). DeepSeek is build atop ChatGPT?
2. Learn that my idea around *GPU benchmark* is already exists. This means, this is a valid problem and people have already worked on it. So, park that idea. 
3. Found a paper titled [Understanding LLMs: A Comprehensive Overview from Training to Inference](https://arxiv.org/pdf/2401.02038). 

**1st January 2024**

1. Completed the blog post on [NVidia NLP/ NER project](https://medium.com/lids-magazine/decoding-nvidias-strategic-focus-insights-from-an-nlp-project-b64de9d707c8).


## 2024

### December

**31st December 2024**
1. Watched [Jhono's AI Art video #2](https://www.youtube.com/watch?v=peTkMmRrxPg&list=PL23FjyM69j910zCdDFVWcjSIKHbSB7NE8&index=2). Convolution layers in CNNs apply filters. When filters are applied, over an image, the resulting image has brighter areas that got activated from that filter. Filters are features that learned. Features are start at random and overtime they learn. Networks learn useful features from the inputs. After applying filters, the subsequent layers to *down-sampling* (e.g. max-pooling). 
    - Key concepts words
        - padding
        - stride 

**30th December 2024**
1. Completed NLP-NER project on Nvidia 

**29th December 2024**

1. [xAIGarage.com](https://xaigarage.com), [hummingbirdlabs.ai] - thought of a new website URL. Use [nbdev](https://nbdev.fast.ai/) to build a portfolio website. 

**26th December 2024**
1. Created a script to download YouTube comments. The goal is to perform sentiment analysis on YouTube video comments.  Scrape only about 500 comments. 


**25th December 2024**
1. Attempted to fuse two music waveforms together. 